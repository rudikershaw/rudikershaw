<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns:th="http://www.w3.org/1999/xhtml">
	<head th:replace="articles/computationalmethod :: head"/>
	<body>
		<div class="header" th:replace="index :: header"></div>
		<br />
		<div class="text-container">
			<p class="date">16/07/2018</p>
			<p class="views"><span th:text="${article.views}"></span> views</p>
		</div>
		<h1 th:text="${article.name}">There Is No AI Apocalypse</h1>
		<hr />
		<img class="coverimg" src="/images/ai-doom-isnt-coming-banner.jpg" alt="20 Something Books" />
		<br />

		<p>Progress in AI is going to bring us some really exciting and interesting technologies. But, you may have heard people discussing the possible dangers we could face with AI in the future.
			Some small but significant portion of those who you hear worrying about the future of AI, are worried about something quite specific. <b>The extinction or subjugation of the entire human race by
			artificial intelligences</b>. It's easy to dismiss these worries as a fantasy that comes from watching Terminator too many times, but there are some very smart people who think this is a very real
			possibility. Some big names include Bill Gates, Elon Musk, Sam Harris, and the late Stephen Hawkings to name only a few.</p>
		<br />
		<p>If lots of very smart people are convinced this is a genuine problem, what is their rationale, and where did this idea come from? Although this has been a fringe concern for many decades,
			the recent spike in attention has come from the release of a particular book. That book is 'Superintelligence' by Nick Bostrom, which became a New York Times best seller in March 2014.</p>
		<br />
		<p>There is no known path towards artificial general intelligence. But Nick Bostrom uses what little we do know to build a picture, make deductions, and prophecise about a future that
			contains general AI. In exploring what this future might look like he paints a rather bleak picture of the dangers this future will face. 'Superintelligence' is a dense three hundred and
			something page book that goes into a great deal of detail about the paths and dangers as we move towards general AI, as well as some broad strategies for dealing wth the situations he sees
			coming.</p>
		<br />
		<p>A lot of thought has been put into this book. He brings in knowledge from topics such as economics, game theory, and computer science to support his argument. It is all very well thought out.
			Fortunately, it is also probably wrong. At risk of over-simplifying his arguments, I am going to attempt to summarise some of his fears, and then go into why his thinking misses some
			fundamentally important considerations.</p>
		<br />
		<h2>A Summary of his Fears</h2>
		<br />
		<p>Bostrom notes that we have no idea what the limit of intelligence is, and we have no reason to believe that we are anywhere near that limit. He asks us to image a spectrum of intelligence from
			an insect, to us, and past us to who knows where. Imagine for a moment a being who's intelligence doesn't just dwarf our greatest thinkers, but dwarfs the entire collective intellect of all people
			working together. This being would very obviously be a threat to us, at least if its goals were not aligned with our own.</p>
		<br />
		<p>The book points out that we wouldn't even need to create this intelligence ourselves. All that would be required would be an AI sufficiently general and smart that it could improve its own
			intellect. Then each time it improves itself it would be capable of an even greater improvement the next time. This would result in what Bostrom calls 'an intelligence explosion'. The explosion is
			the central concern of the book, because it would give us no time to react. It could go from sub-human intelligence, to intellect beyond comprehension in no time at all. You may have also heard this
			referred to as the 'singularity'.</p>
		<br />
		<p>If we managed to get this AI perfectly right, and aligned its goals with some perfect version of our own, it could become a benevolent god to us. But if we get it wrong, and Bostrom points out that
			this is much more likely, we could find ourselves at the mercy of a superintelligence with a strange or unhelpful purpose.</p>
		<br />
		<h2>A Bad Argument</h2>
		<br />
		<p>Although I will get to the point soon and start telling you why this is extremely unlikely, I feel I should quickly cover a bad argument people often make for why we don't have to worry.</p>
		<br />
		<p>People often say that artificial general intelligence might be impossible, but this is very clearly not true. There is a long and colourful history of people saying that certain aspects of
			intelligence will never be possible for a machine, and people who say this are always proved wrong eventually. People have said that machines will never be able to make decisions based on conditions,
			do general purpose calculations, play chess, interpret writing or speech, play go, pass the turing test, have conversations, etc. All of those things are things that machines can do now. And it looks
			as though machines are about to be able to do something that we have only recently heard experts say is impossible, drive cars.</p>
		<br />
		<p>The fact of the matter is that 'general intelligence' doesn't have a hard definition. And so, until it does, there will always be a way for people to convince themselves that machines will never get
			there. The moment you define an aspect of intelligence, or propose a particular task that would qualify as generally intelligent, that problem is usually solved. Generally when people make this argument
			there is more nuance, and they will often talk about emotions, goal setting, perception of self, and the hard problem, but I don't find these arguments convincing. I can't really do these topics credit
			in such a small space. So it's about time I move onto some arguments I do find convincing.</p>
		<br />
		<h2>Why It Will All Be Fine</h2>
		<br />
		<p>I want to make it clear that I am not saying that there is no reason to worry at all, or that we wont get super-intelligences. There are plenty of reasons to be concerned, which I will cover nearer the end.
			Additionally, I believe we will eventually get general purpose super-intelligences. What I do not believe, is that improvements in AI will lead to a sudden take-over by machines. AI is not more dangerous than
			nuclear war. They will not pose some run-away existential threat.</p>
		<br />
		<h4>Isaac Newton's Smart Phone</h4>
		<br />
		<p>'Isaac Newton's Smart Phone' is an analogy for how we tend to ignore the possible limitations of future technologies, written by <a href="https://www.technologyreview.com/s/609048/the-seven-deadly-sins-of-ai-predictions/" target="_blank">Rodney Brooks in an article for MIT Technology Review</a>.
			Imagine taking Isaac Newton from the early 1700s and showing him a smart phone. Show him that the device produces light without fire, sound without obvious movement, records sound and light, retrieves
			information from anywhere in the world, and creates direct links of communication between people who aren't in ear-shot. If you asked Newton how it worked, he couldn't possibly know. It would be
			indistinguishable from magic to him. If you asked him what other properties the phone might have, he might think it would last forever. He would have no concept of electricity, or batteries. He might think it
			could turn lead into gold. Newton believed science was on the cusp of solving that problem. Something as incredible as this device must be able to do it.</p>
		<br />
		<p>Transmutation, the process of changing one chemical or element into another, is a great example of this phenomenon. We have been searching for the ability to change one thing into another from as early as 3500 BC.
			Newton spent a great deal of time contemplating how lead might be turned into gold and might have worried that this ability, in the hands of the wrong people, could result in the de-valuing of English currency.
			He was a member of UK Parliament and Master of the Royal Mint, and alchemy like was banned during his lifetime. Transmutation is a solved problem now. But it has not de-valued gold even a
			little. This is because the energy required to turn lead into gold through nuclear transmutation is much more valuable than the gold it produces.</p>
		<br />
		<p>This worry about de-valuing might seem familiar if you have read Bostrom's book. He devotes many pages to worrying about what might happen to our economy if we create super-intelligences that can
			produce value in amounts that dwarf human labor. Ultimately this concern is the same as Newton's concerns about transmutation. Without understanding how a technology will work, it is easy to assume it will work
			like magic with no limitations. So if artificial super-intelligence will have limitations (and they will), what kind of limitations will they have?</p>
		<br />
		<h4>The Problem with Hard Problems</h4>
		<br />
		<p>One limitation super-intelligences are almost certain to have is that hard problems will still be hard for them. That is a bit of a glib remark, so allow me to explain myself. In computer science we have a
			concept of 'Non-Polynomial' problems, or 'hard problems' (not to be confused with the hard problem of consciousness in philosophy). Hard problems are ones that cannot be solved without a certain level of effort.
			This level of effort scales exponentially with the scope of hard problems. That's probably a little difficult to visualise without an example. The stereotypical example of a hard problem is 'The Travelling Salesman'
			problem.</p>
		<br />
		<p>Imagine a travelling salesman. He has three locations to visit and wants to find which order will keep his distance travelled lowest. The only <b>perfect</b> way to do this is to find all possible routes, measure the
			distance travelled for each, and then pick the shortest. With three locations there are six possible routes. It is trivial for our salesman to work out the shortest route. But if they have four destinations
			there are 24 routes to check. Just one extra location makes it quite a lot more time consuming. Five destinations leaves us with 120 routes, six becomes 720, seven becomes 5040. This quickly gets out of our
			control.</p>
		<br />
		<p>Computers can obviously find the shortest routes for many more destinations than humans can. But, to get the shortest route for 16 destinations a computer has to check <b>20 trillion routes</b>. Even computers
			struggle with those kinds of numbers. A super-intelligence, no matter how smart, will still have trouble with hard problems. What-ever goals a general AI might have, it is likely to encounter hard problems.
			Additionally, it is likely that some aspects of intelligence are hard problems themselves. In this instance the superior speed of electronic components over biological neurones will only offer modest returns.</p>
		<br />
		<h4>The Intelligence Explosion</h4>
		<br />
		<p>Not only would future super-intelligences be at the mercy of hard problems, but the <b>path</b> to creating super-intelligences may also be subject to them. The idea of run-away general AIs relies very heavily on the
			notion of a singularity, or intelligence explosion. Without an intelligence-explosion there would be ample time to deal with any issues we face with the goals, objectives, behavior, etc of our AIs. Do we have any reason
			to believe this explosion is possible or likely? The main argument used to support the possibility of a singularity, is a generalization of Moore's law. This generalization states that technology improves at an exponential
			rate. But, there are a number of problems with this.</p>
		<br />
		<p>The first problem is with assuming that progress in technology as a whole, or in some specific field, can be generalised to AI specifically. The progress of technology as a whole is substantially under-estimated.
			But this is not true of general AI. In 1950, Alan turing predicted we would have general AI by the year 2000. In 1965 Herbert A. Simon predicted we would have it within 20 years. In 1970 Marvin Minsky said we would have it
			within a generation. You can also see this in Bostrom's book where he predicts that certain problems are <b>AI Complete</b>. AI Complete problems are problems that would require a general artificial intelligence. Bostrom
			predicts that Go and fluent conversations will require general artificial intelligences. Google's <a href="https://en.wikipedia.org/wiki/AlphaGo#Match_against_Lee_Sedol" target="_blank">Alpha Go</a> beat one of the worlds
			best go players in 2016, and Google have shown demos of <a href="https://www.youtube.com/watch?v=lXUQ-DdSDoE" target="_blank">conversational AIs</a> this year.</p>
		<br />
		<p>The second problem, which I think is a bigger issue, is that the Moore's law generalization can be used as an argument for why we <b>wont</b> see a singularity. If your argument is that the progress of technology moves at
			some predicable specified rate, then AI should follow exactly that pattern too. Which is to say that there is no reason to believe it will out-pace us, only that it will keep up with our other technologies. If we have other
			technologies as powerful and useful as general AI then it will not have any means of exploiting us.</p>
		<br />
		<h4>General Intelligence</h4>
		<br />
		<p>This fear of an AI take-over is largely limited to <b>general</b> super-intelligence. If an AI does not the ability to think and operate generally it would only be able to work on some highly specific task. The world's best
			chess AI is no threat to us because it only knows chess. It does not understand anything else. In fact, it does not even understand chess except in a very limited way.</p>
		<br />
		<div class="info-box"><b>Bostrom's Orthogonality Thesis</b><br /> Intelligence and final goals are orthogonal: More or less any level of intelligence could in principle be combined with more or less any final goal.</div>
		<br />
		<br />
		<hr />

		<div class="article-footer" th:replace="articles/computationalmethod :: article-footer"></div>

		<div class="text-container"><div id="disqus_thread"></div></div>
		<script type="text/javascript">
			/* * * Configure Disqus Variables * * */
			var disqus_shortname = 'codenerd';
			var disqus_identifier = '1 - There Is No AI Apocalypse';
			var disqus_title = 'There Is No AI Apocalypse';

			/* * * Declare disqus display script * * */
			(function() {
				var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
				dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
				(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
			})();
		</script>
		<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

	</body>
</html>
